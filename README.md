üìà Personal Expense Analyzer & Spending Predictor

This repository contains a data science project that analyzes personal spending habits and forecasts future expenses. The entire project is built around the 7-stage data science process, demonstrating everything from data generation and cleaning to exploratory analysis, hypothesis testing, modeling, and deployment.

The final product is an interactive web application built with Streamlit (streamlit_app_v3.py) that allows users to either generate synthetic data or upload their own expense files for a personalized analysis and forecast.

üöÄ Key Features (Streamlit App)

Dual Data Input:

Generate Data: Create a synthetic 2-year expense dataset with one click.

Upload Data: Upload your own CSV, Excel, or JSON expense files.

Column Mapping: An intuitive interface to map your file's columns (e.g., "My_Cost") to the required fields ("Amount").

Exploratory Data Analysis (EDA): Automatically generates visualizations for:

Spending distribution (Histogram)

Transactions by category (Bar Chart)

Total spending over time (Line Chart)

Statistical Hypothesis Testing:

T-test (Weekend vs. Weekday): Statistically determines if there's a significant difference in spending on weekends vs. weekdays.

T-test (Category vs. Category): Compares the average transaction amount between major categories like 'Food' and 'Personal'.

Predictive Forecasting:

Builds an ARIMA Time-Series Model on the fly.

Displays model accuracy and error metrics ($MAPE$, $RMSE$).

Provides a forecast for the next 1-12 months, complete with charts and data tables.

üõ†Ô∏è How to Run the Project

You can run this project in two ways: as a simple Python script or as the full interactive web app.

Option 1: Run the Streamlit Web App (Recommended)

This is the main, deployed version of the project.

Clone the repository:

git clone [https://github.com/your-username/personal-expense-analyzer.git](https://github.com/your-username/personal-expense-analyzer.git)
cd personal-expense-analyzer


Install the required libraries:

pip install streamlit pandas numpy scipy scikit-learn statsmodels matplotlib seaborn openpyxl


Run the app:

streamlit run streamlit_app_v3.py


Your browser will automatically open to the app, where you can generate or upload data.

Option 2: Run the Explanatory Notebook

If you prefer a more traditional notebook-style experience (e.g., in VS Code, Spyder, or a Jupyter environment), you can use Project_Notebook.py.

Ensure you have the libraries installed (see above).

Open Project_Notebook.py in your IDE.

Run the file cell-by-cell (each block marked with # %%) to see the explanations and outputs for each stage.

üìÇ File Descriptions

streamlit_app_v3.py

What it is: The main, deployable Streamlit web application.

How to use: Run with streamlit run streamlit_app_v3.py.

Features: Combines data generation and user upload into a single, interactive dashboard. This is the final product.

Project_Notebook.py

What it is: An explanatory Python script formatted as a notebook.

How to use: Run cell-by-cell in an interactive-enabled editor (like VS Code).

Features: Walks through all 7 stages with detailed comments and plots, perfect for understanding the process.

personal_expense_analyzer.py

What it is: A single, non-interactive Python script that runs all 7 stages from top to bottom.

How to use: Run with python personal_expense_analyzer.py.

Features: Prints all results and opens plot windows sequentially. Good for a quick, automated run-through of the entire project logic.

my_personal_expenses.csv

What it is: An example synthetic dataset that may be generated by the scripts. You can safely delete this file; the scripts will regenerate it if needed.

üìä The 7 Stages of Data Science in This Project

This project is a practical demonstration of the complete data science lifecycle:

Business Understanding: Define the problem (users don't understand their spending) and the goal (analyze past spending and predict future spending).

Data Acquisition: Implement two methods: generating new synthetic data and loading user-provided files (CSV/Excel/JSON).

Data Preparation: Clean the data (handle nulls, convert data types) and perform feature engineering (extracting Month, Day_of_Week, Is_Weekend from the Date).

Exploratory Data Analysis (EDA): Use matplotlib and seaborn to visualize spending patterns (distributions, categorical counts, time-series plots).

Modeling: Build an ARIMA (Autoregressive Integrated Moving Average) time-series model to learn from past monthly spending data.

Model Evaluation: Test the model's accuracy against a held-back "test set" of data. Calculate key metrics like $RMSE$ (error in dollars) and $MAPE$ (average percentage error).

Deployment: Deploy the entire pipeline as an interactive Streamlit web application (streamlit_app_v3.py) that anyone can use.